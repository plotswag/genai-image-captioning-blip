## Prototype Development for Image Captioning Using the BLIP Model and Gradio Framework

### AIM:
To design and deploy a prototype application for image captioning by utilizing the BLIP image-captioning model and integrating it with the Gradio UI framework for user interaction and evaluation.

### PROBLEM STATEMENT:
The challenge is to create an interactive image captioning tool that uses a pretrained BLIP model to generate captions for images uploaded by users. The application should be able to receive an image, pass it through the BLIP model to generate captions, and then display the generated caption to the user in a user-friendly Gradio interface.
### DESIGN STEPS:


## **STEP 1: Environment Setup**
Install the required libraries such as `gradio`, `transformers`, and `torch` to support the application.
Ensure the environment has access to all necessary model files and dependencies.

---
## **STEP 2: Load the BLIP Model**
Import and initialize the pretrained **BLIP (Bootstrapped Language Image Pretraining)** model using the `transformers` library.
Verify that the model is correctly set up to receive image inputs and generate natural language captions.

---
## **STEP 3: Design the Gradio Interface**
Build an interactive interface using **Gradio** that allows users to upload images.
Once an image is uploaded, the interface will display the caption generated by the BLIP model.

---
## **STEP 4: Testing and Validation**
Run the application and test it with various images.
Evaluate whether the generated captions are accurate, relevant, and informative, reflecting the content of the images effectively.

---
### PROGRAM:
```py
import os
import io
import IPython.display
from PIL import Image
import base64 
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
hf_api_key = os.environ['HF_API_KEY']
# Helper functions
import requests, json

#Image-to-text endpoint
def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_ITT_BASE']):
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL,
                                headers=headers,
                                data=json.dumps(data))
    return json.loads(response.content.decode("utf-8"))
import gradio as gr 

def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image)
    return result[0]['generated_text']

gr.close_all()
demo = gr.Interface(fn=captioner,
                    inputs=[gr.Image(label="Upload image", type="pil")],
                    outputs=[gr.Textbox(label="Caption")],
                    title="Image Captioning with BLIP",
                    description="Caption any image using the BLIP model",
                    allow_flagging="never",
                    examples=["christmas_dog.jpeg", "bird_flight.jpeg", "cow.jpeg"])

demo.launch(share=True, server_port=int(os.environ['PORT1']))
#run this to end server
gr.close_all()
```
### OUTPUT:
![image](https://github.com/user-attachments/assets/c7520e41-f5a6-4e6f-afdd-774c651a6999)

### RESULT:
The prototype application for image captioning was successfully designed and deployed using the BLIP model integrated with the Gradio UI. The system effectively generates relevant captions for uploaded images and provides a user-friendly interface for interaction and evaluation.
  
